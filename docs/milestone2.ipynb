{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: \n",
    "\n",
    "Our software will solve the problem of Forward Automatic Differentiation. Forward Automatic Differentiation allows for accurate and easy gradient calculation for a given function. There are many areas in which Automatic Differentiation is necessary such as back propagation in neural networks (this is more reverse-mode oriented), Jacobian calculation, gradient calculation, finding the local maxima and minima of functions (Newton's method), and optimizing them. With automatic differentiation, it is possible to calculate a derivative to machine precision in a computationally efficient manner. This method allows us to avoid the burden of complexity from symbolic differentation while also having better accuracy than numerical differentation.\n",
    "\n",
    "# Background:\n",
    "\n",
    "### Chain Rule\n",
    "\n",
    "The chain rule allows us to break down a composite function into elementary operations, particularly when calculating the derivative of a function. It is described like so:\n",
    "${dx}=\\frac{df}{dg}\\frac{dg}{dx}$\n",
    "\n",
    "### Elementary operations\n",
    "In forward automatic differentiation, a given function is first broken down into elementary functions. Elementary functions include multiplication, addition, subtraction, division, and other basic functions. Once a given function is \"divided\" into the elementary functions calculated at each step within the function, a graph can be generated where each node is a specific stage in the calculation and each edge is an elementary operation applied to a given node. Applying the derivative chain rule in this graph makes it possible to calculate the gradiants and the \"contributions\" of a given node. \n",
    "\n",
    "### Primal and tangent traces\n",
    "When examining this broken down graph, the primal trace and tangent trace allow us to calculate the intermediate results at a given step in our differentiation calcuation. The forward primal trace of a function computes the value of each intermediate variable $v_j$ at step j while the tangent trace computes the derivatives of these intermediate values, $D_pv_j$ at a given step j. \n",
    "### Seed Vector\n",
    "In order to actually calculate a specific derivative at different steps, we use a seed vector. A seed vector is a \"direction\" vector that allows us to evaluate a derivative with the weighted combination of the seed vector. For example, for a function $f(x_1, x_2)$, we could calculate $\\frac{df}{dx_1}$ with a seed vector of $ p = [1,0]$ or $\\frac{df}{dx_2}$ with a seed vector of $ p = [0, 1]$. \n",
    "### Example \n",
    "The following table displays a simple example of calculating the forward primal trace, tangent trace, and values with seed vectors, from a lab completed in class. The equation $f(x_1, x_2) =  e^{-(sin(x_1)- cos(x_2))^2}$ and we evaluate $f(\\frac{\\pi}{2}, \\frac{\\pi}{3})$:\n",
    "\n",
    "\n",
    "  Forward Primal Trace   | Forward Tangent Trace |  p = $[1,0]^T$    | p = $[0,1]^T$\n",
    "  -----------------------|-----------------------|-------------------|---------------\n",
    "  v0 = x_1 = pi / 2      | Dpv0 = p1             | 1                 | 0\n",
    "  v1 = y_1 =  pi / 3     | Dpv1 = p2             | 0                 | 1\n",
    " | | | \n",
    "  v_2 = sin(v0) = 1      | Dpv2 = cos(v0)*Dpv0   | 0                 | 0\n",
    "  v_3 = cos(v1) = 1/2    | Dpv2 = -sin(v1)*Dpv1  | 0                 | -sqrt(3) / 2\n",
    " | | | \n",
    "  v4 = v2 - v3 = 1/2    | Dpv4 = Dpv2 - Dpv3    | 0                 | sqrt(3) / 2\n",
    " | | | \n",
    "  v5 = v4 * v4 = 1/2     | Dpv5 = 2*v4 *Dpv4     | 0                 | sqrt(3) / 2\n",
    " | | | \n",
    "  v_6 = -1 * v_5 = -1/4  | Dpv6 = -Dpv5          |  0                | -sqrt(3) / 2\n",
    " | | | \n",
    "  v_7 = e^v6 = e^(-1/4)  | Dpv7 = Dpv6 * e^v6    | 0                 | (-sqrt(3) / 2) * e ^(-1/4) \n",
    "\n",
    "### Dual Numbers\n",
    "Dual numbers are another important concept that are extremely useful in forward automatic differentiation. A dual number consists of a real and a dual part (much like complex numbers consist of a real and imaginary part) where $z = a + b\\epsilon$ where a is the real part and b the dual. Dual numbers have notable addition and multiplication properties where, if $z_1 = a_1 + b_1\\epsilon$ and $z_2 = a_2 + b_2\\epsilon$, $z_1 + z_2 = (a_1 + a_2) + (b_1 + b_2)\\epsilon$ and $z_1 * z_2 = (a_1 * a_2) + (a_1b_1 + a_2b_2)\\epsilon$. With respect to automatic differentiation, a dual number can represent a real part $a = f(x)$ and a dual part where $b = f'(x)$. With respect to traces, this real part can represent the primal trace and the dual part the tangent trace. This makes it easy to calculate the derivative and function at a given step, because the addition and multiplication propoerties of the dual numbers as discussed above correctly upholds the chain rule, as proven in lecture with Taylor series expansion. \n",
    "\n",
    "### Jacobian \n",
    "\n",
    "It is often necessary to compute and evaluate derivatives of a function at a given point in order to correctly determine the Jacobian (defined below); thus, forward automatic differentiation can be crucial to such calculations. \n",
    "\n",
    "The Jacobian is a matrix of first order partial derivatives of a given function with respect to the dependent variables. It is often crucial to calculating things such as Newton's method. As an example, take two example functions $x = u^2 - v^2$ and $y = u^2 + v^ 2$. For the Jacobian with respect to this system, we would want: \n",
    "\n",
    "$\\begin{array}{cc}\n",
    "\\frac{dx}{du} & \\frac{dx}{dv} \\\\\n",
    "\\frac{dy}{du} & \\frac{dy}{dv} \\\\\n",
    "\\end{array}$  \n",
    "\n",
    "Which would yield: \n",
    "\n",
    "$\\begin{array}{cc}\n",
    "2u & -2v \\\\\n",
    "2u & 2v \\\\\n",
    "\\end{array}$\n",
    "\n",
    "### Additional Notes\n",
    "\n",
    "Our group thinks that 3Blue1Brown does a great exploration of the topic of automatic differentiation: https://www.youtube.com/watch?v=tIeHLnjs5U8\n",
    "\n",
    "Here is an image from the video which shows a computation graph which represent the operations in a trival NN: \n",
    "<div>\n",
    "<img src=\"https://www.3blue1brown.com/content/lessons/2017/backpropagation-calculus/tree-extended.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "# How to Use Our Package:\n",
    "\n",
    "At this time, you will need to clone our respository to use the package, us `pip install` within the repository to install the requirements and then to install our package for use. Below is an example of the commands in the command line that are needed to install the package.\n",
    "\n",
    "```\n",
    "git clone git@code.harvard.edu:CS107/team48.git\n",
    "cd team48\n",
    "python -m pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "pip install -e .\n",
    "```\n",
    "After this, you should be able to use our given package as specified below. The below example finds the value of the Jacobian for a scalar function and single input (derivative) and returns the result at the specified x as an integer or float depending on the value (In the future, we will be returning an array. However, we currently return an integer or float and only support the Jacobian and derivative calculation for single input and single function cases).\n",
    "```\n",
    "from team48_autodiff_package.AutoDiff import *\n",
    "\n",
    "def func(x):\n",
    "    return 2*x**2 + 1\n",
    "x = 2\n",
    "\n",
    "forward_diff_outs = AutoDiff(func)\n",
    "print(forward_diff_outs(x))\n",
    "```\n",
    "\n",
    "# Software Organization:\n",
    "\n",
    "## Directory Structure and Basic Modules:\n",
    "Our directory follows the below structure. Because we have started integrating our the package to PyPI, our software has some additional files to make this possible. \n",
    "\n",
    "```\n",
    "team48/\n",
    "├── docs\n",
    "│   └── milestone1\n",
    "├── LICENSE\n",
    "├── README.md\n",
    "├── requirements.txt\n",
    "├── pyproject.toml\n",
    "├── tests\n",
    "└── src \n",
    "    ├── team48_autodiff_package\n",
    "    └── team48_autodiff_package.egg-info\n",
    "```\n",
    "\n",
    "Inside of the src folder we have a folder named team48_autodiff_package which includes most of our central modules. This includes:\n",
    "  - dual.py: contains the dual class and the overloaded functions for the DualNumber class\n",
    "  - AutoDiff.py: contains the AutoDiff class that returns the derivative value for a value and function as specified in the example in the 'How to Use' section above.\n",
    "  - ad.py: helper functions for testing and AutoDiff functions \n",
    "\n",
    "## Tests and Running\n",
    "Our tests for the code are in the tests folder. All of the tests can be run with the pytest command. These are integrated into our workflow using GitHub workflows. We have 3 files, test_dual.py, test_forward_ad.py, and test_ad.py that test each of the corresponding files in src/team48_autodiff_package. To run these tests, ensure that you are in the root directory and have installed the necessary dependencies and packages (this is in the how to use section). Then use `pytest` to run the tests. Our tests cover 94% of the code in src/team48_autodiff_package. \n",
    "\n",
    "Our package can be installed as specified in the `How to Use Our Package` section.\n",
    "\n",
    "# Implementation:\n",
    "Below are the two classes that we implemented in our package. The Dual Numbers class serves not only as a core class, but also as a core data stucture necessary for effective forward automatic differentiation. Details about the classes are specified below.\n",
    "- **DualNumber class**:\n",
    "  - Attributes: \n",
    "    - real: a real number part of type float or int\n",
    "    - dual: a dual number part of type float or int\n",
    "  - Methods:\n",
    "    - We overloaded some of the following elementary operations to correctly work for dual numbers as abstractly explained in the background section. The derivative is thus saved as the dual component of the DualNumber. \n",
    "    - Overloaded methods:\n",
    "      - +, -, \\, *, **, ==, __neg__ \n",
    "      -sin()\n",
    "      -cos()\n",
    "      -tan()\n",
    "      -exp()\n",
    "      -log()\n",
    "      -sqrt()\n",
    "\n",
    "- **AutoDiff class**: \n",
    "  - Attributes: \n",
    "    - func: An input function associated with the object\n",
    "    - Methods\n",
    "      - A given instance of the class can be passed a value (at this time a float or int) and will return the evaluated derivative value at the parameter value; the derivative is returned. \n",
    "\n",
    "  Our code also makes use of helper functions that can be found in src/team48_autodiff_package/ad.py. There are 3 functions that serve various testing and implementation purposes that are described below:\n",
    "\n",
    "  - **single_derivative_approximation(fx, x)**:\n",
    "    - This function accepts a function and a value (integer or float) x and closely approximates the derivative at x using $\\frac{f(x) - f(x - h)}{h}$ where $h = 1.e-8$.\n",
    "    - This is used for testing purposes. \n",
    "  -**check_error(test_value, approx)**:\n",
    "    -This function checks if `test_value` (integer or float) is very close to `approx` (integer or float). It checks if the difference between the two values is smaller than 1e-6*(1 + abs(approx)). If the difference is smaller than the error, True is returned; otherwise False is returned.\n",
    "    - This is used for testing purposes to compare single_derivative_approximation() against the true derivative calculation with our AutoDiff class to ensure correctness.\n",
    "  -**single_derivative(fx, x)**\n",
    "    -This function calculates the precise derivative of a function; x is a DualNumber and fx is a function. The derivative is returned.\n",
    "    -This function is called in the AutoDiff class __call__ function when an AutoDiff object is passed a value.\n",
    "\n",
    "We also have a multiVarForward.py function that is a function we are currently working on to implement multi-function and multi-value forward automatic differentiation.\n",
    "\n",
    "Finally, there is a hack_solution.py function that enables the coverage CI test integration through github workflows. This checks to ensure that the tests cover greater than 90% of the code. \n",
    "\n",
    "All requirements can be found in requirements.txt and can be installed with The implementation of these classes requires the installation of numpy. This should be installed if the directions for installation in a previous section with the command 'pip install -r requirements.txt.'\n",
    "    \n",
    "# Upcoming features:\n",
    "- **R^m -> R^n mapping**\n",
    "  - We plan to modify the AutoDiff class to work with functions which accept and return vectors (Numpy array inputs and outputs)\n",
    "- **Parsing a string into a function**\n",
    "  - We plan to allow users to submit a function to create an AutoDiff object by passing a string if they so choose, using the eval() function to parse said string.\n",
    "- **Higher order derivatives**\n",
    "  - We plan to implement evaluation of higher order derivatives through an AutoDiff instance by evaluating the derivative multiple times.\n",
    "## Reverse mode:\n",
    "  - We plan to implement reverse mode evaluation of the derivative. For this, we will add the following classes:\n",
    "- **Operation class**:\n",
    "  - Attributes: \n",
    "    - Enum class where all elementary operations are assigned a number \n",
    "- **Node class**:\n",
    "  - Attributes:\n",
    "    - Optional Parameter Dict\n",
    "    - operation: an Operation type variable representing the operation at the given node \n",
    "    - Value: Dual number value at a given \"step\" in the calculation\n",
    "  - Methods:\n",
    "    - getValue: get the DualNumber value at a given node\n",
    "    - getOperation: get the given operation at a given node\n",
    "- **Graph class**:\n",
    "  - Attributes: \n",
    "    - Nodes: nodes in the graph of type node\n",
    "    - Edges: adjacency list representing edges between different nodes\n",
    "  - Methods:\n",
    "    - reverseAutoDiff: backwards traversal of the graph to calculate the derivative with respect to the given values\n",
    "\n",
    "The core data structures are the classes as discussed above. The Dual numbers will consist of a real number and a dual number (derivative) part. Each node will consist of a operation that occurs at the given node and specific parameters of a dual number type to track the value and derivative in a backwards propogation (we are accounting for reverse mode with this). The graph class will have nodes as discussed above and an adjacency list of edges between these nodes to record the relationships; these will be used to represent our computational graph and are necessary to conduct reverse auto differentiation.\n",
    "\n",
    "We will also overload more operations for dual numbers by declaring custom versions of these functions in the DualNumber class. We will create our own versions of this and default to numpy versions of the functions. We will be using the numpy array and numpy matrix as a fundamental data structures and extend numpy functionality as it allows us to conveniently handle vector input and vector functions. Because handling vector input and numpy data structures are critical to our project we will include the external package numpy. In addition to the functions we already overloaded, we plan on overloading the following functions using numpy:\n",
    "-Inverse trig functions\n",
    "-Exponentials (any base)\n",
    "-Hyperbolic functions\n",
    "-Logarithms (any base)\n",
    "\n",
    "NetworkX is an external package that allows us to visualize graphs. We will use this package to visualize large graph structures. We intend to include this package out of convenience for the user, but it is more of a stretch goal.\n",
    "\n",
    "To handle cases for functions where the input dimensions differ from the output dimensions, we will include checks within nodes to enure that the number of variables and dimension is staying consistent with our defined function. We have discussed a reverseAutoDiff function above and will include that in our library for the function.\n",
    "\n",
    "Given we have enough time, we may also implement a layer class for our graph to help visualize neural net propagation more effectively. \n",
    "\n",
    "# Licensing:\n",
    "We will use the MIT License for our project. Because we are using numpy and possibly NetworkX, we shouldn't have to deal with any issues of patents. We are okay with people making and distributing closed source versions of our code.\n",
    "    \n",
    "You can find a copy of this license here: https://choosealicense.com/licenses/mit/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1235ae0a98321feaf3f94ea3bcd3bfdba6dacffe25ee88cd8ffc3ed2c28eb22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
