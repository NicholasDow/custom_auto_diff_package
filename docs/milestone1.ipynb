{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps-14bv37i02"
   },
   "source": [
    "# Introduction: \n",
    "\n",
    "Our software will solve the problem of Forward Automatic Differentiation. Forward Automatic Differentiation allows for accurate and easy gradient calculation for a given function. There are many areas in which Automatic Differentiation is necessary such as back propagation in neural networks (this is more reverse-mode oriented), finding the local maxima and minima of functions, and optimizing them. With automatic differentiation, it is possible to calculate a derivative to machine precision in a computationally efficient manner. This method allows us to avoid the burden of complexity from symbolic differentation while also having better accuracy than numerical differentation.\n",
    "\n",
    "# Background:\n",
    "\n",
    "### Chain Rule\n",
    "\n",
    "The chain rule allows us to break down a composite function into elementary operations, it is described like so:\n",
    "${dx}=\\frac{df}{dg}\\frac{dg}{dx}$\n",
    "\n",
    "### Elementary operations\n",
    "In forward automatic differentiation, a given function is first broken down into elementary functions. Elementary functions include multiplication, addition, subtraction, division, and other basic functions. Once a given function is \"divided\" by the elementary functions, a graph can be generated where each node is a specific stage in the calculation and each edge is an elementary operation applied to a given node. Applying the derivative chain rule in this graph to get the gradiants and calculate the \"contributions\" of a given node. \n",
    "\n",
    "### Primal and tangent traces\n",
    "When examining this broken down graph, the primal trace and tangent trace allow us to calculate the intermediate results at a given step in our differentiation calcuation. The forward primal trace of a function computes the value of each intermediate variable $v_j$ at step j while the tangent trace computes the derivatives of these intermediate values, $D_pv_j$ at a given step j. \n",
    "### Seed Vector\n",
    "In order to actually calculate a specific derivative at different steps, we use a seed vector. A seed vector is a \"direction\" vector that allows us to evaluate a derivative with the weighted combination of the seed vector. For example, for a function $f(x_1, x_2)$, we could calculate $\\frac{df}{dx_1}$ with a seed vector of $ p = [1,0]$ or $\\frac{df}{dx_2}$ with a seed vector of $ p = [0, 1]$. \n",
    "### Example \n",
    "The following table displays a simple example of calulcating the forward primal trace, tangent trace, and values with seed vectors, from a lab completed in class. The equation $f(x_1, x_2) =  e^{-(sin(x_1)- cos(x_2))^2}$ and we evaluate $f(\\frac{\\pi}{2}, \\frac{\\pi}{3})$:\n",
    "\n",
    "\n",
    "  Forward Primal Trace   | Forward Tangent Trace |  p = $[1,0]^T$      | p = $[0,1]^T$\n",
    "  -----------------------|-----------------------|-------------------|---------------\n",
    "  v0 = x_1 = pi / 2      | Dpv0 = p1             | 1                 | 0\n",
    "  v1 = y_1 =  pi / 3     | Dpv1 = p2             | 0                 | 1\n",
    " | | | \n",
    "  v_2 = sin(v0) = 1      | Dpv2 = cos(v0)*Dpv0   | 0                 | 0\n",
    "  v_3 = cos(v1) = 1/2    | Dpv2 = -sin(v1)*Dpv1  | 0                 | -sqrt(3) / 2\n",
    " | | | \n",
    "  v4 = v2 - v3 |= 1/2    | Dpv4 = Dpv2 - Dpv3    | 0                 | sqrt(3) / 2\n",
    " | | | \n",
    "  v5 = v4 * v4 = 1/2     | Dpv5 = 2*v4 *Dpv4     | 0                 | sqrt(3) / 2\n",
    " | | | \n",
    "  v_6 = -1 * v_5 = -1/4  | Dpv6 = -Dpv5          |  0                | -sqrt(3) / 2\n",
    " | | | \n",
    "  v_7 = e^v6 = e^(-1/4)  | Dpv7 = Dpv6 * e^v6    | 0                 | (-sqrt(3) / 2) * e ^(-1/4) \n",
    "\n",
    "### Dual Numbers\n",
    "Dual numbers are another important concept that are extremely useful in forward automatic differentiation. A dual number consists of a real and a dual part (much like complex numbers consist of a real and imaginary part) where $z = a + bϵ$ where a is the real part and b the dual. Dual numbers have notable addition and multiplication properties where, if $z_1 = a_1 + b_1ϵ$ and $z_2 = a_2 + b_2ϵ$, $z_1 + z_2 = (a_1 + a_2) + (b_1 + b_2)\\epsilon$ and $z_1 * z_2 = (a_1 * a_2) + (a_1b_1 + a_2b_2)\\epsilon$. With respect to automatic differentiation, a dual number can represent a real part $a = f(x)$ and a dual part where $b = f'(x)$; with respect to traces, this real part can represent the primal trace and the dual part the tangent trace. This makes it easy to calculate the derivative and function at a given step, because the addition and multiplication propoerties of the dual numbers as discussed above correctly upholds the chain rule, as proven in lecture with Taylor series expansion. \n",
    "\n",
    "### Jacobian \n",
    "\n",
    "It is often necessary to compute and evaluate derivatives of a function at a given point in order to correctly determine the Jacobian (defined below); thus, forward automatic differentiation can be crucial to such calculations. \n",
    "\n",
    "The Jacobian is a matrix of first order partial derivatives of a given function with respect to the dependent variables. It is often crucial to calculating things such as Newton's method. As an example, take two example functions $x = u^2 - v^2$ and $y = u^2 + v^ 2$. For the Jacobian with respect to this system, we would want: \n",
    "\n",
    "$\\begin{array}{cc}\n",
    "\\frac{dx}{du} & \\frac{dx}{dv} \\\\\n",
    "\\frac{dy}{du} & \\frac{dy}{dv} \\\\\n",
    "\\end{array}$  \n",
    "\n",
    "Which would yield: \n",
    "\n",
    "$\\begin{array}{cc}\n",
    "2u & -2v \\\\\n",
    "2u & 2v \\\\\n",
    "\\end{array}$\n",
    "\n",
    "### Additional Notes\n",
    "\n",
    "Our group thinks that 3Blue1Brown does a great exploration of this topic: https://www.youtube.com/watch?v=tIeHLnjs5U8\n",
    "\n",
    "Here is an image from the video which shows a computation graph which represent the operations in a trival NN: \n",
    "<div>\n",
    "<img src=\"https://www.3blue1brown.com/content/lessons/2017/backpropagation-calculus/tree-extended.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "Our package should be able to implement this graph and eventually be able to use it to train and make inferences.\n",
    "\n",
    "# How to Use AutoDiffPackage:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;We are calling our package AutoDiffPackage. Our goal is for this package to be accessible from at least a test server. The users will have to have NumPy and NetworkX installed inorder to use AutoDiffPackage. NumPy is fundamental to the calculations while NetworkX is for graph visualization.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;A user should be able to both parse an equation from a string or represent that equation/NN via objects/activation functions created and composed using our package. Users should also be able to execute this graph structure given input data. Users should be able to also directly calculate gradients using the forward AD class and pasing it a function and inputs. Users should have the ability to choose which part of their package is best for their use case be it forward AD which requires less space or backward passes on a graph data structure which allow for gradient descent.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "from AutoDiffPackage import Graph, AutoDiff\n",
    "```\n",
    "Here are examples of how we envision the code being used\n",
    "```\n",
    "example_func = lambda x: return 2*x**2 + 1\n",
    "func = '2*x**2 + 1'\n",
    "\n",
    "equation = Graph(example_func) # or Graph(func)\n",
    "graph_diff = equation.autodiff()\n",
    "\n",
    "# forward diff you have different options for forward AD\n",
    "vec = np.array([1])\n",
    "forward_diff_outs = AutoDiff(func, vec)\n",
    "print(forward_diff)\n",
    "\n",
    "diffs = AutoDiff(func)\n",
    "print(diffs)\n",
    "outs = diffs(vec)\n",
    "print(outs)\n",
    "```\n",
    "\n",
    "# Software Organization:\n",
    "\n",
    "We think that the design philosophy of PyTorch is excellent, and will try our best to emulate their clean and thoughtful stucturing of code as the package grows.\n",
    "\n",
    "For now our directory will follow the following structure: \n",
    "\n",
    "```\n",
    "team48/\n",
    "├── docs\n",
    "│   └── milestone1\n",
    "├── LICENSE\n",
    "├── README.md\n",
    "├── tests\n",
    "└── src \n",
    "```\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Inside of the src folder we plan on including a module called Autodiff for calculating the gradient of a given input function. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;We also plan on having a graph-oriented module called Graph that will be used to build a graph representation of the process and will build out this representation. Our tests suite will be found in the tests directory of our folders and our package will be distributed via Pytools and PyPI.\n",
    "\n",
    "# Implementation:\n",
    "\n",
    "At this time nothing is set in stone, and we are doing our best to plan out how we want to implement the assignment. We think we will need the following classes for our implementation:\n",
    "    \n",
    "- **Dual Numbers**:\n",
    "  - Attributes: \n",
    "    - real: a real number part of type float or int\n",
    "    - dual: a dual number part of type float or int\n",
    "  - Methods:\n",
    "    - We will overload elementary operations (addition, multiplication, division, and subtraction, sin, etc) to correctly work for dual numbers. \n",
    "- **Operation class**:\n",
    "  - Attributes: \n",
    "    - Enum class where all elementary operations are assigned a number \n",
    "- **Node class**:\n",
    "  - Attributes:\n",
    "    - Optional Parameter Dict\n",
    "  - operation: an Operation type variable representing the operation at the given node \n",
    "  - Value: Dual number value at a given \"step\" in the calculation\n",
    "- **Graph class**:\n",
    "  - Attributes: \n",
    "    - Nodes: nodes in the graph of type node\n",
    "    - Edges: adjacency list representing edges between different nodes\n",
    "- **AD class**: \n",
    "  - Attributes: \n",
    "    - This calss will accept a function and possibly accept values to be evaluated. \n",
    "    - Methods\n",
    "      - This class will have a function grad() to calculate the gradient of the given function. \n",
    "      - This class will also make sure to account for the altered addition in multiplication (this will mainly be addressed by the dual number class but will be considered here too).\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The core data structures are the classes as discussed above. The Dual numbers will consist of a real number and a dual number (derivative) part. Each node will consist of a operation that occurs at the given node and specific parameters of a dual number type to track the value and derivative in a backwards propogation (we are accounting for reverse mode with this). The graph class will have nodes as discussed above and an adjacency list of edges between these nodes to record the relationships; these will be used to represent our computational graph and are necessary to conduct reverse auto differentiation.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;We will overload basic operations for dual numbers by declaring custom versions of these functions in the DualNumber class. We will create our own versions of this and default to numpy versions of the functions. We will be using the numpy array and numpy matrix as a fundamental data structures and extend numpy functionality as it allows us to conveniently handle vector input and vector functions. Because handling vector input and numpy data structures are critical to our project we will include the external package numpy. We plan on overloading the following functions using numpy:\n",
    "\n",
    "```sin()```\n",
    "\n",
    "```cos()```\n",
    "\n",
    "```tan()```\n",
    "\n",
    "```exp()```\n",
    "\n",
    "```log()```\n",
    "\n",
    "```sqrt()```\n",
    "\n",
    "NetworkX is an external package that allows us to visualize graphs. We will use this package to visualize large graph structures. We intend to include this package out of convenience for the user, but it is more of a stretch goal.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;To handle cases for functions where the input dimensions differ from the output dimensions, we will include checks within nodes to enure that the number of variables and dimension is staying consistent with our defined function. We have discussed a gradient function above and will include that in our library for the function.\n",
    "\n",
    "We may also implement a layer class should we have enough time\n",
    "\n",
    "# Licensing:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;We will use the MIT License for our project. Because we are using numpy and possibly NetworkX, we shouldn't have to deal with any issues of patents. We are okay with people making and distributing closed source versions of our code.\n",
    "    \n",
    "You can find a copy of this license here: https://choosealicense.com/licenses/mit/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1235ae0a98321feaf3f94ea3bcd3bfdba6dacffe25ee88cd8ffc3ed2c28eb22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
